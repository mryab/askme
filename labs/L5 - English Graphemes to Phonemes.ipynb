{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs,re,os\n",
    "\n",
    "\n",
    "def create_vocabulary(data):\n",
    "    vocab = {}\n",
    "    for line in data:\n",
    "        for item in line:\n",
    "            if item in vocab:\n",
    "                vocab[item] += 1\n",
    "            else:\n",
    "                vocab[item] = 1\n",
    "    vocab_list = sorted(vocab)\n",
    "    vocab = dict([(x, y+1) for (y, x) in enumerate(vocab_list)])\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def symbols_to_ids(symbols, vocab):\n",
    "    ids = [vocab.get(s) for s in symbols]\n",
    "    return ids\n",
    "\n",
    "\n",
    "def split_to_grapheme_phoneme(inp_dictionary):\n",
    "    graphemes, phonemes = [], []\n",
    "    for line in inp_dictionary:\n",
    "        split_line = re.split('[ _]', line.strip())\n",
    "        if len(split_line) > 1:\n",
    "            graphemes.append(list(split_line[0]))\n",
    "            phonemes.append(split_line[1:])\n",
    "    return graphemes, phonemes\n",
    "\n",
    "\n",
    "def collect_pronunciations(dic_lines):\n",
    "    dic = {}\n",
    "    for line in dic_lines:\n",
    "        lst = line.strip().split()\n",
    "        if len(lst) > 1:\n",
    "            if lst[0] not in dic:\n",
    "                dic[lst[0]] = [\" \".join(lst[1:])]\n",
    "            else:\n",
    "                dic[lst[0]].append(\" \".join(lst[1:]))\n",
    "    return dic\n",
    "\n",
    "\n",
    "def split_dictionary(train_path, valid_path=None, test_path=None):\n",
    "    source_dic = codecs.open(train_path, \"r\", \"utf-8\").readlines()\n",
    "    train_dic, valid_dic, test_dic = [], [], []\n",
    "    if valid_path:\n",
    "        valid_dic = codecs.open(valid_path, \"r\", \"utf-8\").readlines()\n",
    "    if test_path:\n",
    "        test_dic = codecs.open(test_path, \"r\", \"utf-8\").readlines()\n",
    "\n",
    "    dic = collect_pronunciations(source_dic)\n",
    "    \n",
    "    for i, word in enumerate(dic):\n",
    "        for pronunciations in dic[word]:\n",
    "            train_dic.append(word + ' ' + pronunciations)\n",
    "    return train_dic, valid_dic, test_dic\n",
    "\n",
    "\n",
    "def prepare_g2p_data(train_path, valid_path, test_path):\n",
    "    train_dic, valid_dic, test_dic = split_dictionary(train_path, valid_path, test_path)\n",
    "    train_gr, train_ph = split_to_grapheme_phoneme(train_dic)\n",
    "    valid_gr, valid_ph = split_to_grapheme_phoneme(valid_dic)\n",
    "\n",
    "    ph_vocab = create_vocabulary(train_ph)\n",
    "    gr_vocab = create_vocabulary(train_gr)\n",
    "            \n",
    "    train_ph_ids = [symbols_to_ids(line, ph_vocab) for line in train_ph]\n",
    "    train_gr_ids = [symbols_to_ids(line, gr_vocab) for line in train_gr]\n",
    "    valid_ph_ids = [symbols_to_ids(line, ph_vocab) for line in valid_ph]\n",
    "    valid_gr_ids = [symbols_to_ids(line, gr_vocab) for line in valid_gr]\n",
    "\n",
    "    return train_gr_ids, train_ph_ids, valid_gr_ids, valid_ph_ids, gr_vocab, ph_vocab, test_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers import Dense, Activation, Masking, Embedding, Bidirectional,BatchNormalization\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import TensorBoard,EarlyStopping\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "tf.set_random_seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_gr_ids, train_ph_ids, valid_gr_ids, valid_ph_ids, gr_vocab, ph_vocab, test_lines=\\\n",
    "prepare_g2p_data('train1.txt',None,'test.csv')\n",
    "ph_rev_vocab=dict((x,y) for (y,x) in ph_vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test=[]\n",
    "for line in test_lines:\n",
    "    if not line.startswith('Id'):\n",
    "        test.append(symbols_to_ids(line.strip().split(',')[1],gr_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "padded_tr_gr_ids=sequence.pad_sequences(train_gr_ids,padding='post',truncating='post')\n",
    "num_timesteps=padded_tr_gr_ids.shape[1]\n",
    "padded_tr_ph_ids=sequence.pad_sequences(train_ph_ids,maxlen=num_timesteps,padding='post',truncating='post')\n",
    "\n",
    "inp_voc_size=len(gr_vocab)\n",
    "outp_voc_size=len(ph_vocab)\n",
    "test_padded=sequence.pad_sequences(test,maxlen=num_timesteps,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_gr=to_categorical(padded_tr_gr_ids,num_classes=inp_voc_size+1).reshape(padded_tr_gr_ids.shape[0],padded_tr_gr_ids.shape[1],inp_voc_size+1)\n",
    "tr_ph=to_categorical(padded_tr_ph_ids,num_classes=outp_voc_size+1).reshape(padded_tr_ph_ids.shape[0],padded_tr_ph_ids.shape[1],outp_voc_size+1)\n",
    "ts_gr=to_categorical(test_padded,num_classes=inp_voc_size+1).reshape(test_padded.shape[0],test_padded.shape[1],inp_voc_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_gr[:,:,0]=0\n",
    "ts_gr[:,:,0]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tr(a):\n",
    "    return ','.join(sorted('='.join(list(map(lambda x:str(x)[:10] if type(x)!=dict else tr(x)[:10],\n",
    "                                             item))) for item in a.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def funct(params,i=None,save=False):\n",
    "    print('Params testing:\\n',params,end='\\n')\n",
    "    model = Sequential()\n",
    "    \n",
    "    if params['unit']=='LSTM':\n",
    "        unit=LSTM\n",
    "    else:\n",
    "        unit=GRU\n",
    "        \n",
    "    model.add(Bidirectional(unit(params['num_1'],recurrent_dropout=params['dropout_1'], \n",
    "                                 return_sequences=True,implementation=2),merge_mode=params['merge_mode'],\n",
    "                           input_shape=(num_timesteps,inp_voc_size+1)))\n",
    "    \n",
    "    for _ in range(params['enc_layers']):\n",
    "        model.add(unit(params['num_2'],recurrent_dropout=params['dropout_2'],\n",
    "                       return_sequences=True,implementation=2))\n",
    "\n",
    "    model.add(Dense(outp_voc_size+1))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    if params['opt']['type']=='adadelta':\n",
    "        opt=keras.optimizers.adadelta()\n",
    "    elif params['opt']['type']=='adam':\n",
    "        opt=keras.optimizers.adam()\n",
    "    else:\n",
    "        opt=keras.optimizers.rmsprop(lr=params['opt']['learning_rate'])\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(tr_gr,tr_ph,batch_size=params['batch_size'],epochs=100,verbose=2,\n",
    "              callbacks=[EarlyStopping(min_delta=0.0001,monitor='loss',patience=5),TensorBoard(log_dir='./logs/'+tr(params))])\n",
    "    if save:\n",
    "        pred=model.predict_classes(ts_gr,verbose=2)\n",
    "        res=[\"_\".join([ph_rev_vocab[elem] for elem in sym[sym!=0]]) for sym in pred]\n",
    "        with open('hopt_pred'+str(i)+'.csv','w+') as outp:\n",
    "            outp.write(\"Id,Transcription\\n\")\n",
    "            for i,word in enumerate(res):\n",
    "                outp.write(str(i+1)+','+word+'\\n')\n",
    "    else:\n",
    "        loss, score = model.evaluate(tr_gr,tr_ph,verbose=2)\n",
    "    return {'loss':loss,'status':STATUS_OK,'score':score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials,space_eval\n",
    "space={\n",
    "    'enc_layers':hp.randint('enc_layers',5),\n",
    "    'dec_layers':hp.randint('dec_layers',5),\n",
    "    'merge_mode':hp.choice('merge_mode',['concat','ave','sum']),\n",
    "    'unit':hp.choice('unit',['LSTM','GRU']),\n",
    "    'num_1':hp.choice('num_1',[128,256,512,1024]),\n",
    "    'dropout_1':hp.uniform('dropout_1',0.01,0.1),\n",
    "    'num_2':hp.choice('num_2',[128,256,512,1024]),\n",
    "    'dropout_2':hp.uniform('dropout_2',0.01,0.1),\n",
    "    'batch_size':hp.choice('batch_size',[64,128,256,512,1024]),\n",
    "    'opt':hp.choice('opt',[\n",
    "        {'type':'adadelta'},\n",
    "        {'type':'adam'},\n",
    "        {'type':'rmsprop','learning_rate':hp.uniform('learning_rate',0.0001,0.001)}\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials=Trials()\n",
    "best=fmin(funct,space,algo=tpe.suggest,trials=trials,max_evals=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trials1=list(trials)\n",
    "trials1.sort(key=lambda x:x['result']['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_params=list(map(lambda x:space_eval(space,x),\n",
    "                     [{key:value[0] for key,value in tr['misc']['vals'].items() if len(value)>0}\n",
    "                    for tr in trials1[:10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,params in enumerate(best_params):\n",
    "    funct_best(params,save=True,i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
