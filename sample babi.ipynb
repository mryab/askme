{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout, Masking\n",
    "from keras.layers import add, dot, concatenate\n",
    "from keras.layers import GRU, BatchNormalization, Concatenate, RepeatVector, LSTM\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from functools import reduce\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split('(\\W+)', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines, only_supporting=False,n=True):\n",
    "    '''Parse stories provided in the bAbi tasks format\n",
    "    If only_supporting is true, only the sentences\n",
    "    that support the answer are kept.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if n:\n",
    "            nid, line = line.split(' ', 1)\n",
    "            nid = int(nid)\n",
    "            if nid == 1:\n",
    "                story = []\n",
    "        else:\n",
    "            pass\n",
    "        if '?' in line:\n",
    "            if n:\n",
    "                q, a, supporting = line.split('\\t')\n",
    "                a = tokenize(a.replace(',',' '))\n",
    "            else:\n",
    "                q=line\n",
    "            q = tokenize(q)\n",
    "            substory = None\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            if n:\n",
    "                data.append((substory, q, a))\n",
    "            else:\n",
    "                data.append((substory,q))\n",
    "            story.append('')\n",
    "            if not n:\n",
    "                story=[]\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    '''Given a file name, read the file,\n",
    "    retrieve the stories,\n",
    "    and then convert the sentences into a single story.\n",
    "    If max_length is supplied,\n",
    "    any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "    with open(f) as file:\n",
    "        data = parse_stories(file.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n",
    "\n",
    "def zarr(w,l):\n",
    "    a=np.zeros(l)\n",
    "    a[w]=1\n",
    "    return a\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen, answer_maxlen,n=False):\n",
    "    X = []\n",
    "    Xq = []\n",
    "    Y = []\n",
    "    if n:\n",
    "        for story, query in data:\n",
    "            x = [word_idx[w] for w in story]\n",
    "            xq = [word_idx[w] for w in query]\n",
    "            X.append(x)\n",
    "            Xq.append(xq)\n",
    "        return (pad_sequences(X, maxlen=story_maxlen),\n",
    "            pad_sequences(Xq, maxlen=query_maxlen))\n",
    "    for story, query, answer in data:\n",
    "        x = [word_idx[w] for w in story]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        xa = [word_idx[w] for w in answer]\n",
    "        # let's not forget that index 0 is reserved\n",
    "#         y = np.zeros(len(word_idx) + 1)\n",
    "#         y[word_idx[answer]] = 1\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(xa)\n",
    "    return (pad_sequences(X, maxlen=story_maxlen),\n",
    "            pad_sequences(Xq, maxlen=query_maxlen),\n",
    "            pad_sequences(Y, maxlen=answer_maxlen))\n",
    "\n",
    "challenges = {\n",
    "        \"1\": \"qa1_single-supporting-fact\",\n",
    "        \"2\": \"qa2_two-supporting-facts\",\n",
    "        \"3\": \"qa3_three-supporting-facts\",\n",
    "        \"4\": \"qa4_two-arg-relations\",\n",
    "        \"5\": \"qa5_three-arg-relations\",\n",
    "        \"6\": \"qa6_yes-no-questions\",\n",
    "        \"7\": \"qa7_counting\",\n",
    "        \"8\": \"qa8_lists-sets\",\n",
    "        \"9\": \"qa9_simple-negation\",\n",
    "        \"10\": \"qa10_indefinite-knowledge\",\n",
    "        \"11\": \"qa11_basic-coreference\",\n",
    "        \"12\": \"qa12_conjunction\",\n",
    "        \"13\": \"qa13_compound-coreference\",\n",
    "        \"14\": \"qa14_time-reasoning\",\n",
    "        \"15\": \"qa15_basic-deduction\",\n",
    "        \"16\": \"qa16_basic-induction\",\n",
    "        \"17\": \"qa17_positional-reasoning\",\n",
    "        \"18\": \"qa18_size-reasoning\",\n",
    "        \"19\": \"qa19_path-finding\",\n",
    "        \"20\": \"qa20_agents-motivations\",\n",
    "        \"MCTest\": \"MCTest\",\n",
    "        \"19changed\": \"19changed\",\n",
    "        \"joint\": \"all_shuffled\", \n",
    "        \"sh1\": \"../shuffled/qa1_single-supporting-fact\",\n",
    "        \"sh2\": \"../shuffled/qa2_two-supporting-facts\",\n",
    "        \"sh3\": \"../shuffled/qa3_three-supporting-facts\",\n",
    "        \"sh4\": \"../shuffled/qa4_two-arg-relations\",\n",
    "        \"sh5\": \"../shuffled/qa5_three-arg-relations\",\n",
    "        \"sh6\": \"../shuffled/qa6_yes-no-questions\",\n",
    "        \"sh7\": \"../shuffled/qa7_counting\",\n",
    "        \"sh8\": \"../shuffled/qa8_lists-sets\",\n",
    "        \"sh9\": \"../shuffled/qa9_simple-negation\",\n",
    "        \"sh10\": \"../shuffled/qa10_indefinite-knowledge\",\n",
    "        \"sh11\": \"../shuffled/qa11_basic-coreference\",\n",
    "        \"sh12\": \"../shuffled/qa12_conjunction\",\n",
    "        \"sh13\": \"../shuffled/qa13_compound-coreference\",\n",
    "        \"sh14\": \"../shuffled/qa14_time-reasoning\",\n",
    "        \"sh15\": \"../shuffled/qa15_basic-deduction\",\n",
    "        \"sh16\": \"../shuffled/qa16_basic-induction\",\n",
    "        \"sh17\": \"../shuffled/qa17_positional-reasoning\",\n",
    "        \"sh18\": \"../shuffled/qa18_size-reasoning\",\n",
    "        \"sh19\": \"../shuffled/qa19_path-finding\",\n",
    "        \"sh20\": \"../shuffled/qa20_agents-motivations\",\n",
    "        \"all\": \"all\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting stories for the challenge: all\n",
      "-\n",
      "Vocab size: 175 unique words\n",
      "Story max length: 199 words\n",
      "Query max length: 12 words\n",
      "Number of training stories: 191714\n",
      "Number of test stories: 17164\n",
      "-\n",
      "Here's what a \"story\" tuple looks like (input, query, answer):\n",
      "(['Fred', 'is', 'either', 'in', 'the', 'school', 'or', 'the', 'park', '.', 'Mary', 'went', 'back', 'to', 'the', 'office', '.'], ['Is', 'Mary', 'in', 'the', 'office', '?'], ['yes'])\n",
      "-\n",
      "Vectorizing the word sequences...\n",
      "-\n",
      "inputs: integer tensor of shape (samples, max_length)\n",
      "inputs_train shape: (191714, 199)\n",
      "inputs_test shape: (17164, 199)\n",
      "-\n",
      "queries: integer tensor of shape (samples, max_length)\n",
      "queries_train shape: (191714, 12)\n",
      "queries_test shape: (17164, 12)\n",
      "-\n",
      "answers: binary (1 or 0) tensor of shape (samples, vocab_size)\n",
      "answers_train shape: (191714, 3, 175)\n",
      "answers_test shape: (17164, 3, 175)\n",
      "-\n",
      "Compiling...\n"
     ]
    }
   ],
   "source": [
    "challenge_type = 'all'\n",
    "challenge = challenges[challenge_type]+'_{}.txt'\n",
    "\n",
    "DATA_DIR='dmn_simple/data/babi/en-10k'\n",
    "\n",
    "print('Extracting stories for the challenge:', challenge_type)\n",
    "train_stories = get_stories(os.path.join(DATA_DIR,challenge.format('train')),max_length=200)\n",
    "test_stories = get_stories(os.path.join(DATA_DIR,challenge.format('test')),max_length=200)\n",
    "\n",
    "vocab = set()\n",
    "for story, q, answer in train_stories + test_stories:\n",
    "    vocab |= set(story + q + answer)\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories)))\n",
    "answer_maxlen = max(map(len, (x for _, _, x in train_stories + test_stories)))\n",
    "\n",
    "print('-')\n",
    "print('Vocab size:', vocab_size, 'unique words')\n",
    "print('Story max length:', story_maxlen, 'words')\n",
    "print('Query max length:', query_maxlen, 'words')\n",
    "print('Number of training stories:', len(train_stories))\n",
    "print('Number of test stories:', len(test_stories))\n",
    "print('-')\n",
    "print('Here\\'s what a \"story\" tuple looks like (input, query, answer):')\n",
    "print(train_stories[0])\n",
    "print('-')\n",
    "print('Vectorizing the word sequences...')\n",
    "\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "inputs_train, queries_train, answers_train = vectorize_stories(train_stories,\n",
    "                                                               word_idx,\n",
    "                                                               story_maxlen,\n",
    "                                                               query_maxlen,\n",
    "                                                              answer_maxlen)\n",
    "answers_train=np.array([to_categorical(i,vocab_size) for i in answers_train])\n",
    "inputs_test, queries_test, answers_test = vectorize_stories(test_stories,\n",
    "                                                            word_idx,\n",
    "                                                            story_maxlen,\n",
    "                                                            query_maxlen,\n",
    "                                                           answer_maxlen)\n",
    "answers_test=np.array([to_categorical(i,vocab_size) for i in answers_test])\n",
    "print('-')\n",
    "print('inputs: integer tensor of shape (samples, max_length)')\n",
    "print('inputs_train shape:', inputs_train.shape)\n",
    "print('inputs_test shape:', inputs_test.shape)\n",
    "print('-')\n",
    "print('queries: integer tensor of shape (samples, max_length)')\n",
    "print('queries_train shape:', queries_train.shape)\n",
    "print('queries_test shape:', queries_test.shape)\n",
    "print('-')\n",
    "print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)')\n",
    "print('answers_train shape:', answers_train.shape)\n",
    "print('answers_test shape:', answers_test.shape)\n",
    "print('-')\n",
    "print('Compiling...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR='dmn_simple/data/glove/glove.6B/'\n",
    "EMBEDDING_DIM=100\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in word_idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# placeholders\n",
    "input_sequence = Input((story_maxlen,))\n",
    "question = Input((query_maxlen,))\n",
    "\n",
    "hid_dim=50\n",
    "\n",
    "context_rec=Masking()(input_sequence)\n",
    "context_rec=Embedding(input_dim=vocab_size,output_dim=100,weights=[embedding_matrix])(context_rec)\n",
    "context_rec=BatchNormalization()(context_rec)\n",
    "context_rec=Bidirectional(GRU(hid_dim,return_sequences=True),merge_mode='concat')(context_rec)\n",
    "context_rec=BatchNormalization()(context_rec)\n",
    "context_rec=Dropout(0.15)(context_rec)\n",
    "\n",
    "question_rec=Masking()(question)\n",
    "question_rec=Embedding(input_dim=vocab_size,output_dim=100,weights=[embedding_matrix])(question_rec)\n",
    "question_rec=BatchNormalization()(question_rec)\n",
    "question_rec=Bidirectional(GRU(hid_dim),merge_mode='concat')(question_rec)\n",
    "question_rec=BatchNormalization()(question_rec)\n",
    "question_rec=RepeatVector(inputs_train.shape[1])(question_rec)\n",
    "question_rec=Dropout(0.15)(question_rec)\n",
    "\n",
    "answer=concatenate([context_rec,question_rec])\n",
    "\n",
    "answer=GRU(hid_dim)(answer)\n",
    "answer=BatchNormalization()(answer)\n",
    "answer=RepeatVector(answer_maxlen)(answer)\n",
    "answer=Dropout(0.2)(answer)\n",
    "answer=GRU(hid_dim,return_sequences=True)(answer)\n",
    "answer=BatchNormalization()(answer)\n",
    "answer=Dropout(0.2)(answer)\n",
    "answer = Dense(vocab_size)(answer)\n",
    "answer = Activation('softmax')(answer)\n",
    "\n",
    "# build the final model\n",
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 191714 samples, validate on 17164 samples\n",
      "Epoch 1/500\n",
      "191714/191714 [==============================] - 113s - loss: 0.0405 - acc: 0.9820 - val_loss: 0.0852 - val_acc: 0.9727\n",
      "Epoch 2/500\n",
      "191714/191714 [==============================] - 114s - loss: 0.0404 - acc: 0.9819 - val_loss: 0.0867 - val_acc: 0.9726\n",
      "Epoch 3/500\n",
      "191714/191714 [==============================] - 114s - loss: 0.0408 - acc: 0.9818 - val_loss: 0.0868 - val_acc: 0.9729\n",
      "Epoch 4/500\n",
      " 60000/191714 [========>.....................] - ETA: 77s - loss: 0.0389 - acc: 0.9827"
     ]
    }
   ],
   "source": [
    "# train\n",
    "model.fit([inputs_train, queries_train], answers_train,\n",
    "          batch_size=2000,\n",
    "          epochs=500,\n",
    "          validation_data=([inputs_test, queries_test], answers_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voc={v:k for k,v in word_idx.items()}\n",
    "voc[0]=\"\"\n",
    "\n",
    "def ans(st):\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    tst=[(flatten(story), q) for story, q in parse_stories(st.split(\"\\n\"),n=False)]\n",
    "    inputs_val, queries_val = vectorize_stories(tst,word_idx,story_maxlen,query_maxlen,answer_maxlen,n=True)\n",
    "    res=model.predict([inputs_val,queries_val])\n",
    "    return \" \".join(list(map(lambda x:voc[x],np.argmax(res,axis=2)[0]))).strip(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "st=\"\"\"Mary went to the kitchen.\n",
    "Mary got the apple there.\n",
    "Mary went to the bathroom.\n",
    "Mice are in a rectangle.\n",
    "John took the milk.\n",
    "John went to the hallway.\n",
    "Mary discarded the apple.\n",
    "John took the apple there.\n",
    "John went to the garden.\n",
    "Cats are red.\n",
    "John is a pink rectangle.\n",
    "John picked up the football.\n",
    "What is John carrying?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "st=\"\"\"Mary is red.\n",
    "Mary is a rectangle.\n",
    "Mary went to Fred.\n",
    "What color is Mary?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'garden'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17164/17164 [==============================] - 2s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.095633176408447748, 0.97586024678467431]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([inputs_test, queries_test], answers_test,batch_size=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191714/191714 [==============================] - 20s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.016675329478823154, 0.99338765165340381]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([inputs_train, queries_train], answers_train,batch_size=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
